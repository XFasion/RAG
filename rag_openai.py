import os 
from llama_index.core import (
    StorageContext,
    load_index_from_storage,
    Settings
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from openai import OpenAI
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embed_model = HuggingFaceEmbedding(
    model_name="nlpai-lab/KURE-v1",
    device="cpu",
    token=HF_TOKEN
)

Settings.embed_model = embed_model

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ
storage_context = StorageContext.from_defaults(persist_dir="./faiss_conversation_db_expanded")
index = load_index_from_storage(storage_context)

# Retriever ì„¤ì •
retriever = index.as_retriever(similarity_top_k=3)
retriever.embed_model = embed_model

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=OPENAI_API_KEY)

# ì§ˆë¬¸ì— ë‹µí•˜ëŠ” í•¨ìˆ˜ ì •ì˜
def query_rag(question):
    nodes = retriever.retrieve(question)
    knowledge = "\n".join(node.text for node in nodes)

    print("\nâœ… ê²€ìƒ‰ëœ ë¬¸ì„œ:\n", knowledge)

    prompt = f"""
    ë‹¤ìŒ ëŒ€í™”ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ê°„ë‹¨íˆ ë‹µí•´ì£¼ì„¸ìš”. ëª¨ë¥´ë©´ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.

    ëŒ€í™”: {knowledge}

    ì§ˆë¬¸: {question}
    ë‹µë³€:
    """

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=200,
        temperature=0
    )

    return response.choices[0].message.content.strip()

# ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
questions = [
    "ì‚¬ìš©ìê°€ ë¬´ì„œì›Œí•˜ëŠ” ë™ë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ì‚¬ìš©ìê°€ ì¢‹ì•„í•˜ëŠ” ì˜í™” ì¥ë¥´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
    "ì‚¬ìš©ìê°€ ì‹«ì–´í•˜ëŠ” ìŒì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ì‚¬ìš©ìê°€ ë‘ë ¤ì›Œí•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ì‚¬ìš©ìê°€ ë§¤ìš´ ìŒì‹ì„ ì˜ ë¨¹ë‚˜ìš”?"
]

for question in questions:
    answer = query_rag(question)
    print(f"\nâ“ì§ˆë¬¸: {question}\nğŸ‘‰ë‹µë³€: {answer}")
