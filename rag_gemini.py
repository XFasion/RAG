import os
from dotenv import load_dotenv
from llama_index.core import (
    StorageContext,
    load_index_from_storage,
    Settings
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import google.generativeai as genai

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")
GOOGLE_API_KEY = os.getenv("GEMINI_API_KEY")

# âœ… Gemini ì„¤ì •
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel("models/gemini-1.5-flash-001")

# âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embed_model = HuggingFaceEmbedding(
    model_name="nlpai-lab/KURE-v1",
    device="cpu",
    token=HF_TOKEN
)
Settings.embed_model = embed_model

# âœ… FAISS ë²¡í„° DB ë¡œë“œ
storage_context = StorageContext.from_defaults(persist_dir="./faiss_conversation_db_expanded")
index = load_index_from_storage(storage_context)
retriever = index.as_retriever(similarity_top_k=3)
retriever.embed_model = embed_model

# âœ… ì§ˆë¬¸ â†’ ê²€ìƒ‰ â†’ ì‘ë‹µ

def query_rag(question):
    nodes = retriever.retrieve(question)
    knowledge = "\n".join(node.text for node in nodes)

    print("\nâœ… ê²€ìƒ‰ëœ ë¬¸ì„œ:\n", knowledge)

    prompt = f"""ë‹¤ìŒ ëŒ€í™”ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ê°„ë‹¨íˆ ë‹µí•´ì£¼ì„¸ìš”. ëª¨ë¥´ë©´ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.

ëŒ€í™”: {knowledge}

ì§ˆë¬¸: {question}
ë‹µë³€:"""

    response = model.generate_content(prompt)
    return response.text.strip()

# âœ… í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    questions = [
        "ì‚¬ìš©ìê°€ ë¬´ì„œì›Œí•˜ëŠ” ë™ë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì‚¬ìš©ìê°€ ì¢‹ì•„í•˜ëŠ” ì˜í™” ì¥ë¥´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ì‚¬ìš©ìê°€ ì‹«ì–´í•˜ëŠ” ìŒì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì‚¬ìš©ìê°€ ë‘ë ¤ì›Œí•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì‚¬ìš©ìê°€ ë§¤ìš´ ìŒì‹ì„ ì˜ ë¨¹ë‚˜ìš”?"
    ]

    for question in questions:
        answer = query_rag(question)
        print(f"\nâ“ì§ˆë¬¸: {question}\nğŸ‘‰ë‹µë³€: {answer}")
